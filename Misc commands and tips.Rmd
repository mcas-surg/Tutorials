---
title: "Misc R commands and tips"
author: "Matthew Castelo, matthew.castelo@medportal.ca"
output: word_document
number_sections: yes
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
```
### - Steps for cleaning data 

1. Load data and check type of each variable
```{r, eval=FALSE}
dataset <- read_csv('C:/Users/matth/Documents/data.csv') #Tibble format
str(dataset)
```
2. Convert variables as needed, including factors with factor labels and levels

3. Check for duplicates
```{r, eval=FALSE}
sum(duplicated(dataset))
```
4. Use summary(dataset) to look at each variable and examine for outliers, nonsensical values. Consider setting to missing

5. Look for missing with:
```{r, eval=FALSE}
colSums(is.na(dataset))
md.pattern(dataset, rotate.names = TRUE)
```
6. Remove or impute missing data. For remove use:
```{r, eval=FALSE}
na.omit(dataset)
#Or
dataset_1 <- dataset %>% 
  na.omit()
```
7. Consider using ggpairs(dataset) to look for preliminary relationships between variables 

### - In RStudio, the shortcut for a 'pipe' in Tidyverse AKA '%>%' is CTR-SHIFT-M

### - Greater than or equal to (>=), less than or equal to (<=)

### - To see a dataset simply type name in and hit enter, or try the following:
```{r, eval=FALSE}
View(dataset) #Table format
head(dataset, 10) #First 10 rows
tail(dataset, 10) #Last 10 rows
```
### - To see underlying dataset structure use:
```{r, eval=FALSE}
glimpse(dataset) #Tidyverse format
str(dataset)
```
### - Changing multiple column names very efficiently
```{r, eval=FALSE}
#First, to get the existing column names
colnames(dataset)

#Then, create a vector of the new column names that are wanted
colnames(dataset) <- c('new_var_1', 'new_var_2', 'new_var_3')
```
### - Fast descriptive statistics. For all variables:
```{r, eval=FALSE}
summary(dataset)
```
For a single variable:
```{r, eval=FALSE}
summary(dataset$variable)
```
### - To select multiple different values of a variable use %in% c(value1, value2, etc):
```{r, eval=FALSE}
dataset %>% 
  filter(variable %in% c(1, 2, 3))
```
### - if/else. Logical statement following by action if true, then action if false.
```{r, eval=FALSE}
dataset %>% 
  mutate(new_variable = ifelse (old_variable == 'value', 1, 0))
```
To use a straight if or else (this will only work on elements, not vectors):
```{r, eval=FALSE}
if (logical statement) { code }
	else { code }
```
### - Tips for missing data

To find number of missing in each column:
```{r, eval=FALSE}
colSums(is.na(dataset))
#For patterns in all the data
md.pattern(dataset, rotate.names = TRUE)
```
To replace values with missing:
```{r, eval=FALSE}
dataset$variable[dataset$variable==99] <- NA
#Or, using dplyr
replace(variable, condition, replacement)
#Over multiple columns, use sapply
variables <- c('var_1', 'var_2', 'var_3')
dataset[variables] <- sapply(dataset[variables], function(x) ifelse(x==0, NA, x))
```
To remove missing values:
```{r, eval=FALSE}
dataset_1 <- dataset %>% 
  na.omit()
```
Good piece of code to find patterns in missing data (for one variable):
```{r, eval=FALSE}
dataset %>%
  mutate(miss_variable = is.na(variable)) %>%
	group_by(miss_variable) %>%
	summarize_all(median, na.rm=TRUE)
```
### - Converting to factors

For a single variable:
```{r, eval=FALSE}
dataset$variable <- factor(dataset$variable, levels = c(), labels = c())
```
To convert multiple variables to factors at the same time:
```{r, eval=FALSE}
categorical_vars <- c("variable_1","variable_2","variable_3")
dataset[categorical_vars] <- lapply(dataset[categorical_vars], factor)
```
To drop empty factor levels:
```{r, eval=FALSE}
droplevels()
```
### - Converting from character to date
```{r, eval=FALSE}
dataset_1 <- dataset %>% 
  mutate(new_date = as.Date(old_date, format = '%Y-%m-%d'))
```
The following symbols are used to represent various dates:

%d - day as number
%a - abbreviated weekday (Mon)
%A - unabbreviated weekday (Monday)
%m - month as number
%b - abbreviated month (Jan)
%B - unabbreviated month (January)
%y - two-digit year (07)
%Y - four-digit year (2007)

### - Converting all character variables to factors, and in general performing the same function on multiple variables in dplyr - across()
```{r, eval=FALSE}
dataset %>% 
  mutate(across(where(is.character), factor))

#Can create new columns with pre-defined names
iris %>%
  group_by(Species) %>%
  summarise(across(starts_with("Sepal"), mean, .names = "mean_{.col}"))
```
### - Perform a simple left join
```{r, eval=FALSE}
merged_dataset <- dataset_1 %>% 
  left_joint(dataset_2, by='study_id')
```
### - Writing a function
```{r, eval=FALSE}
my_function <- function(x) {
  x <- x^2
  return(x)
  }

my_function(dataset)
```
### - Running simple loops. The code below will run the loop 6 times:
```{r, eval=FALSE}
x <- c() #Define empty elements before using them in the loop

for (i in 1:6) { 
  x[i] <- i^2
 	}
print(x)
```
### - Splitting one dataset into multiple based on the levels of a factor
```{r, eval=FALSE}
#The output will be a list of datasets - can access them using [[1]], [[2]], etc
split_dataset <- dataset %>% 
  mutate(splitting_var = factor(splitting_var)) %>% 
  group_split(splitting_var)

#Re-combining them:
combined_dataset <- rbind(split_dataset[[1]], split_dataset[[2]], split_dataset[[3]])
```
### - Robust (sandwich-type) variance estimator when using propensity score weights
- https://www.coursera.org/lecture/crash-course-in-causality/data-example-in-r-Ie48W
- a regular GLM model is used with the weight statement
- the sandwich package is used to estimate the covariance structure of the beta coefficients (that's the vcovHC function)
- the diag function gives only the variances for each beta (diagonals in the covariance matrix)
- the square root of the variances gives the standard error
- the confidence intervals are then manually calculated from the standard errors
```{r, eval=FALSE}
x <- glm(highpain ~ treatment,weights=weight_ate, data=dataset3, family=binomial(link='log'))
beta <- coef(x)
SE <- sqrt(diag(vcovHC(x,type='HC0')))
lci <- (beta[2]-1.96*SE[2])
uci <- (beta[2]+1.96*SE[2])
estimate <- c(beta,lci,uci)
exp(estimate)

#p-value
z_stat <- beta/SE
p_values <- pchisq(z_stat^2, 1, lower.tail=FALSE)
p_values
```
For robust variance estimator with the cluster defined use slightly different vcov function:
```{r, eval=FALSE}
SE <- sqrt(diag(vcovCL(model_output, cluster=dataset$cluster_variable)))
```
### - Frequency tables
```{r, eval=FALSE}
table(dataset$variable)
#Or, for 2x2 tables
table(dataset$var_1, dataset$var_2)
#To make a properly formatted confusion matrix (2x2 table) may need to change the order:
table(dataset$var_1, dataset$var_2)[2:1, 2:1]
```
To get nicely formatted contingency 2x2 tables using the gmodels package: 
```{r, eval=FALSE}
CrossTable(x = dataset$var_1, y = dataset$var_2)
#Can add option chisq=TRUE 
```
### - Function to normalize a variable
```{r, eval=FALSE}
#Min-max normalization
normalize <- function(x) {
	return((x - min(x)) / (max(x) - min(x)))
  }
```
an alternative to min-max normalization is z-score standardization:
```{r, eval=FALSE}
z_normalize <- function(x) {
	return((x - mean(x)) / (sd(x)))
  }
```
### - Splitting dataset into train and test sets
```{r, eval=FALSE}
train <- dataset %>% 
  slice_sample(prop=0.25) #splitting into 25% for train set and rest for test

train_index <- as.numeric(rownames(train))

test <- dino_normalized[-train_index,]
```
### - Estimating relative risks and risk differences instead of odds ratios:
```{r, eval=FALSE}
#RR
model <- glm(outcome ~ exposure, data=dataset, family=binomial(link='log'))
summary(model)

#RD
model <- glm(outcome ~ exposure, data=dataset, family=binomial(link='identity'))
summary(model)
```
### - Indexing [row,column]

dataset[2:5] will select columns 2 to 5 in the dataset of interest. These are equivalent:
```{r, eval=FALSE}
dataset$variable1	
dataset[,1]
```
Can also use indexing for logical statements and changing values/variables:
```{r, eval=FALSE}
dataset$age[dataset$age > 99] <- NA #This sets age>99 equal to missing
```
Can use more complex indexing to remove rows only for certain commands. For example, if you want to run a ttest for age only on treatment 1 and treatment 2, removing treatment 3:
```{r, eval=FALSE}
t.test(age ~ treatment, data=dataset[!dataset$treatment==3,])
```
Can do the same but more complex - say now there are 5 treatments and only want to compare the first 2:
```{r, eval=FALSE}
t.test(age ~ treatment, data=dataset[!dataset$treatment %in% c(3,4,5),]) 
```
### - Using TableOne package. CreateTableOne gives the package all the information and options are selected in the print function. Make sure categorical variables are factors.
```{r, eval=FALSE}
all_vars <- c('var_1','var_2','var_3')
continuous_vars <- c('var_1','var_2')

table_1 <- CreateTableOne(vars=all_vars, strata="exposure_variable", data=dataset, test=TRUE) #this will give p-values
table_1_done <- print(table_1, nonnormal=continuous_vars, #this will give median/IQR for variables you choose
		smd=TRUE, #Standardized mean differences
		minMax=TRUE, #Will give range instead of IQR)
	write.csv(table_1_done, file="C:/Users/matth/Documents/R/file_name.csv")
```
### - Simple univariate tests
```{r, eval=FALSE}
wilcox.test(outcome ~ predictor, data=dataset, na.rm=TRUE, #if you want to remove NAs
  paired=TRUE) #for paired you have to have all of one pair then the next in order
t.test(same as above)
fisher.test(dataset$var_1, dataset$var_2)
chisq.test(same as above)
```
### - Saving a publication quality ggplot
```{r, eval=FALSE}
#Run plot you want to save, then:
ggsave('C:/Users/matth/Documents/R/sample_plot.png',
       dpi=300, dev='png', height=10, width=10, units="cm")
```
### - Adding horizontal/vertical lines and labels to ggplot2
```{r, eval=FALSE}
+geom_segment(aes(x=0, xend=20, y=5, yend=5))
+annotate("text",x=1,y=1,hjust=0,#This aligns to the left of the text
          label="What you want it to say")
#Adding axis and titles
+labs(x="X-axis title",y="Y-axis title",title="Title of plot")
```
### - Adding manual rectangle in ggplot
```{r, eval=FALSE}
+ geom_rect(aes(xmin = 0, xmax = 10, ymin = 2, ymax = 4),
            fill = "transparent", color = "red", size = 1)
```
### - Manually setting x and y axis breaks in ggplot
```{r, eval=FALSE}
+ scale_x_continuous(breaks=seq(0,100,5)) #Will go from 0 to 100 in steps of 5
```
### - Log transformation for axis when negative values are present
```{r, eval=FALSE}
#Since the log of a negative number does not exist, need GGALLIN package
+ scale_y_continuous(trans=pseudolog10_trans)

#Alternatively, if negative values don't make sense can impute all negatives as zero. Then add a constant of 1 to all values. That works because log(1) = 0 so the zeros appropriately remain as zero
add_one_log <- function(x) {
  x <- log(x+1)
  x <- round(x, digits=2)
  return(x)
}

dataset %>% 
  mutate(across(everything(), add_one_log))
```
### - Adding strings on the end of variable values using paste function
```{r, eval=FALSE}
paste(variable_name, "text or number you want on the end", sep="_") 
#This will put and underscore and then whatever you have in the middle on the end of the variable value
```
### - Manual code for confidence intervals (regular variance estimator, not robust)
```{r, eval=FALSE}
x <- glm(response ~ predictor, data=dataset, family=binomial(link=logit))
beta <- coef(x)
SE <- sqrt(diag(vcov(x)))
lci <- (beta[2]-1.96*SE[2])
uci <- (beta[2]+1.96*SE[2])
estimate <- c(beta,lci,uci)
exp(estimate) #If you need to exponentiate 
```
### - Removing strings from values and manipulating text. Use the 'stringr' package and works well within dplyr. 

For example, removing the $ sign from a variable:
```{r, eval=FALSE}
dataset$new_variable <- str_remove(dataset$old_variable, "$")
```
To remove white spaces before and after: 
```{r, eval=FALSE}
str_trim()
```
To remove all of a group of characters: 
```{r, eval=FALSE}
str_remove_all("-") #This will remove dashes
```
To convert upper case to lower case: 
```{r, eval=FALSE}
str_to_lower() 
```
To replace characters in a string, for example dashes with spaces: 
```{r, eval=FALSE}
str_replace_all(variable, "-", " ")
```
To remove characters at the beginning or end of a string:
```{r, eval=FALSE}
str_sub(variable, start=1, end=4) #Will remove everything after position 4
```
Removing non-ASCII chars (e.g. emojis)
```{r, eval=FALSE}
gsub("[^\x01-\x7F]", "", variable)
```
Good functions are also contained in the 'tm' package
```{r, eval=FALSE}
stripWhitespace(variable) #Stripping extra spaces between words
removeWords(variable, stopwords('english'))  #Removing English stopwords
```
Concatenate two character variables into one
```{r, eval=FALSE}
new_variable = str_c(var_1, var_2, sep = "-") #Separating with a dash
```

### - Text-based machine learning methods

A corpus contains all the words contained in all documents/observations, after cleaning. Cleaning usually includes removing puncuation, digits, stop words, converting to lower case, and removing extra white spaces. The 'tm' package is used. 
```{r, eval=FALSE}
library(tm)
corpus <- Corpus(VectorSource(dataset$text_variable)) #Creating corpus
```

Now the frequency of each word in each document is summed to create a document-term sparse matrix
```{r, eval=FALSE}
matrix <- DocumentTermMatrix(corpus)
```

Sparse terms can be removed to reduce high dimensional data
```{r, eval=FALSE}
new_matrix <- removeSparseTerms(DocumentTermMatrix(corpus), 0.99)
```

However, the format is in a sparse matrix for memory purposes. For calculations, an actual matrix needs to be created
```{r, eval=FALSE}
as.matrix(new_matrix)
```

Sometimes, zero counts can cause problems and those documents/observations need to be removed. However it can be difficult because the text labels are usually in a separate vector. This code will solve that problem
```{r, eval=FALSE}
library(slam)
#Identifying documents that have zero occurrences and will throw errors later
matrix_cleaned <- docu_term_matrix[row_sums(docu_term_matrix) > 0, ]
#Extracting the row names of the empty documents
index <- matrix_cleaned$dimnames$Docs

#Filtering where the variable ID above equals to non-empty documents and therefore the labels will still match the document term matrix
dataset_cleaned <- original_dataset %>% 
  mutate(ID = as.character(row_number())) %>% 
  filter(ID %in% index)
```

A better method is to go one step further and create a weighted Tf-Itf (term-frequency/inverse-term-frequency) matrix. This takes into account how often a word is used in the entire corpus, so common words among all observations are weighted less, and rare but important words weighted more. 
```{r, eval=FALSE}
weighted <- weightTfIdf(docu_term_matrix, normalize = TRUE)
matrix_weighted <- as.matrix(weighted)
```

Creating a word cloud
```{r, eval=FALSE}
library(wordcloud)
freq <- colSums(matrix_weighted)

set.seed(2020)
wordcloud(names(freq), freq)
```

Now the matrix weighted by Tf-Itf can be passed to k-means clustering, classification, etc.

### - Computing intra-class or intra-cluster correlation coefficients froma mixed effects linear model

Using the nlme package. ICC can only be reliably calculated for random-intercepts models. It is defined as:

Variance(random intercept) / Variance(random intercepts) + Variance(residuals)

In a fixed effect only linear model, the residuals are the only source of variance estimated in the model. In a random intercepts model, there are two sources, the random intercept and residuals. The ICC is the proportion of total variance explained by the random intercept. 
```{r, eval = FALSE}
library (nlme)

r_intercept_model <- lme(response ~ predictor_1 + predictor_2,
                            random = ~ 1 | patient_id, #Random intercepts
                            data = dataset)

summary(r_intercept_model)

#Look at the standard deviation of the random intercept and the residuals in the random effects section of the model output. Square them to get the variances then follow the standard equation for ICC
```

### - Displaying variance - covariance matrices in mixed models using nlme

Using lme(), you can choose to display 3 types of VarCov matrices using the getVarCov() function. The default is to display the random effects. Say you had random intercepts and random slopes for 2 covariates, the VarCov matrix would be a 3x3 matrix. This is called the G matrix for random effects. 

If you specify type = "conditional", the VarCov matrix for residuals for individuals is displayed. This is the VarCov matrix for measurement occasion 1, 2, ..., n for individuals. This is the R matrix. Unless repeated measures are specified, this is just sigma*(Identity matrix) because normally measurements are assumed to be independent. 

If you specify type = "marginal", this is the VarCov matrix for the response at measurment occasions for the population. This basically takes into account all the random and fixed effects and tells you how they are related across measurement occasions for the entire population. If you built a random intercept and random slopes mixed model to approximate an unstructured covariance matrix for repeated measures, your marginal VarCov matrix would be analogous to the conditional (default) VarCov matrix in the repeated measures model (gls() function). 
```{r, eval=FALSE}
#Compound symmetry repeated measures
cs_model <- gls(response ~ predictor,
                           corr = corSymm(, form = ~ measure_occasion | id),
                           weights = varIdent(form = ~ 1 | measure_occasion),
                           data = dataset)

getVarCov(cs_model)

#Note for repeated measures we used the gls() function not lme() and the default behaviour of getVarCov() is different for the two model objects. For gls it prints the R matrix. For lme it prints the G matrix (random effects VarCov matrix).

#This should be analogous to the following:


```


### - Computing VIFs for multicollinearity using 'car' package
```{r, eval=FALSE}
model <- lm(response ~ predictor, data=dataset)
vif(model)
```
### - Linear discriminant analysis and PCA with biplots

Linear discriminant analysis (LDA)
```{r, eval=FALSE}
library(ggord)
library(MASS)

#First can consider scaling the data to help with normality assumption
iris_1 <- iris %>% 
  mutate(across(-Species, scale))

lda_model <- lda(Species ~ ., data=iris_1) #Using all predictors (must be continuous)
#Each linear discriminant axis tried to maximize separateness and are independent from each other - the number of linear discriminant axes = 1 - (# of classes). LD1 and LD2 in the model output gives the beta coefficients to create the first and second linear discriminant axes, respectively

lda_preds <- predict(lda_model)
#Gives the predicted class, posterior probabilities, and points on the linear discriminant axes for each observation

#To create a simple biplot with each observation plotted on the first 2 linear discriminant axes using the ggord package (more detailed code in PCA below)
ggord(lda_model, iris$Species)+
  theme_classic()

#The LDA function also can perform leave-one-out cross validation. The following code performs this and gives an example where the macro-averaged precision, recall, and F1 score can be calculated for a 3-class example (more complicated than simple sensitivity/specificity)

lda_cv <- lda(class ~ ., data=dataset, CV=TRUE) #leave one out cross validation
table <- table(lda_cv$class, dataset$class) #Make sure confusion matrix is formatted correctly

precision_1 = table[1,1] / (table[1,1]+table[1,2]+table[1,3]) 
precision_2 = table[2,2] / (table[2,1]+table[2,2]+table[2,3])
precision_3 = table[3,3] / (table[3,1]+table[3,2]+table[3,3])
precision_avg = (precision_1 + precision_2 + precision_3)/3

recall_1 = table[1,1] / (table[1,1]+table[2,1]+table[3,1])
recall_2 = table[2,2] / (table[1,2]+table[2,2]+table[3,2])
recall_3 = table[3,3] / (table[1,3]+table[2,3]+table[3,3])
recall_avg = (recall_1 + recall_2 + recall_3)/3

f1_avg = (2*precision_avg*recall_avg)/(precision_avg+recall_avg) 
```
PCA
```{r, results='hide'}
pca_out <- prcomp(iris[!names(iris) == 'Species'], 
                  scale = TRUE, 
                  retx=TRUE) #This gives the loadings (coefficients)

# These are the loadings, the coefficients connecting the features and the principal components
pca_out$rotation

# The rotated data
pca_out$x

#Looking at the proportion of variance explained by each PC
pve <- ((pca_out$sdev)^2)/sum(((pca_out$sdev)^2))
plot(pve, xlab="Principal Component", ylab="Proportion of
         Variance Explained ", ylim=c(0,1) ,type="b")
plot(cumsum(pve), xlab="Principal Component", ylab="
       Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
       type="b")

cumsum(pve) #Cumulative sum of variance explained for increasing # of principal components

#Can use ggord for PCA as well to make biplots
library(ggord)
library(ggsci) #For nice colors

labels <- list(Sepal.Length=' `Sepal length` ', #Changing arrow names
               Sepal.Width=' `Sepal width` ',
               Petal.Length=' `Petal length` ',
               Petal.Width=' `Petal width` ')

ggord(pca_out, iris$Species, 
      vec_lab=labels, #To have nice names on arrows
      vec_ext=3, #To control how long the arrows are (they remain proportional)
      alpha_el=0.4, #Transparency of the cluster bubbles 
      size=4) + #Size of the data points
  theme_classic()+
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks=element_blank())+
  theme(legend.position = 'top')+
  scale_fill_jco()+
  scale_color_jco()
```
### - Converting data from long to wide format (and back) using dplyr
```{r, eval=FALSE}
#The pivot_longer() and pivot_wider() functions
dataset %>% 
  pivot_wider(names_from = label_variable,
              values_from = values_variable)

dataset %>% 
  pivot_longer(cols = c(var_1, var_2, var_3),
               names_to = 'overall_var',
               values_to = 'value')
```
### - Creating combined ggplots with inset tables using cowplot function
```{r, eval=TRUE}
library(cowplot)
library(grid)
library(gridExtra)
library(gtable) 

#In this example, 1 ggplot is combined with a simple table and simple formatting
ggplot_1 <- iris %>% 
  ggplot(aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point(size=2)+
  theme_minimal()+
  labs(title = "Sepal Characteristics")

mean_characteristics <- iris %>% #This summarizes mean characteristic and reshapes table
  group_by(Species) %>% 
  summarize(across(everything(), mean)) %>% 
  pivot_longer(cols = c(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width),
               names_to = 'characteristic',
               values_to = 'value') %>% 
  pivot_wider(names_from = Species,
              values_from = value)

#Now converting to a table that can be inserted into the figure
iris_table <- tableGrob(mean_characteristics,
                         cols=c('Characteristics', 'Setosa', 'Versicolor', 'Verginica'),
                         theme = ttheme_minimal(core=list(fg_params=list(hjust=0, x=0.1)),
                                                rowhead=list(fg_params=list(hjust=0, x=0)),
                                                colhead=list(fg_params=list(hjust=0, 
                                                                            x=0.1))), 
                                                rows = NULL)

#This adds a box around the table
iris_table <- gtable_add_grob(iris_table, grobs = rectGrob(gp = gpar(fill = NA, lwd = 2)),
                              t = 1, #We want the box to start at the first row 
                              b = nrow(iris_table), #Want the box to end at the end of table
                              l = 1, #Start box at first column
                              r = ncol(iris_table)) #Same as above

#Adding another box for the column headings
iris_table <- gtable_add_grob(iris_table, grobs = rectGrob(gp = gpar(fill = NA, lwd = 2)),
                              t = 1, b = 1, l = 1, r = ncol(iris_table))


#Now creating the overall figure by combining the two ggplots and formatted table. The way this works is the plot area is a grid 1x1 and the bottom left corner of each figure is given an x/y coordinate and a height/width
ggdraw() +
  draw_plot(ggplot_1, x = 0, y = 0.5, width = 1, height = 0.5)+
  draw_plot(iris_table, x = 0, y = 0, width = 1, height = 0.5)+
  draw_plot_label(label = c('A', 'B'), size = 12,
                  x = c(0, 0), y = c(1, 0.5))
```
```{r, eval=FALSE}
ggsave('C:/Users/matth/Documents/R/folder/plot_1.png',
       dpi=300, dev='png', height=20, width=20, units="cm")
```

A simpler method for just plots that don't include tables is the ggarrange() function in the 
ggpubr package
```{r, eval=FALSE}
library(ggpubr)

ggarrange(plot_1, plot_2, plot_3, plot_4,
          nrow=2, ncol=2)

ggsave('C:/Users/matth/Documents/R/folder/plot_1.png',
       dpi=300, dev='png', height=20, width=20, units="cm")
```

### - K-means clustering, hierarchical clustering and choosing optimal cluster size

K-means
```{r, results=FALSE}
library(cluster)

#Scaling data
iris_cluster <- iris %>% 
  mutate(across(-Species, scale))

#Finding optimal number of clusters using a skree plot
wss <- 0

set.seed(2020)
# For 1 to 3 cluster centers
for (i in 1:5) {
  km_model <- kmeans(iris_cluster[1:4], i, nstart=20)
  # Save total within sum of squares to wss variable
  wss[i] <- km_model$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:5, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

#Look for the 'elbow' on the skree plot for a possible optimal cluster size


#Trying to identify number of clusters using silhouette plot (alternative to skree plot)
#Running a loop to find optimal clusters with silhouette plot
avg_sil_width <- c()
cluster_size <- c()

set.seed(2020)
for (q in 2:5) {
  kmeans_for_silhouette <- kmeans(iris_cluster[1:4], q, nstart=20)
  silhouette_model <- silhouette(kmeans_for_silhouette$cluster, dist(iris_cluster[1:4]))
  sil_sum <- summary(silhouette_model)
  avg_sil_width[q] <- sil_sum$avg.width
  cluster_size[q] <- q
}

optimal_cluster <- data.frame(cluster_size = na.omit(cluster_size),
                              width = na.omit(avg_sil_width))

optimal_cluster %>% 
  slice_max(width, n=1) %>% 
  View()

# Plotting 2 clusters on silhouette plot. The width refers to how 'alike' observations are within each cluster. Negative values indicate that observation is more similar to a different cluster from the one to which it was assigned.
set.seed(2020)
kmeans_for_silhouette <- kmeans(iris_cluster[1:4], 3, nstart=20)
silhouette_model <- silhouette(kmeans_for_silhouette$cluster,dist(iris_cluster[1:4]))
summary(silhouette_model)

plot(silhouette_model, col = c(1:3))

#Evaluating performance of 3 clusters using k means
set.seed(2020)
km_model <- kmeans(iris_cluster[1:4], 3, nstart=20)

table(km_model$cluster, iris_cluster$Species)
```

Hierarchical clustering
```{r, results=FALSE}
iris_dist <- dist(iris[1:4])
h_clust_model <- hclust(iris_dist, method='complete')

plot(h_clust_model, labels=iris$Species, main=" Complete Linkage ", 
     xlab ="", sub ="", ylab ="")

#If we want to cut the tree at three clusters
cut_tree <- cutree(h_clust_model, k=3)

table(cut_tree, iris$Species )
```

### - Heatmaps

The heatmap() function has a strange format by default. It wants each observation to have a unique name/ID and these must be the actual row names rather than another column. This can be done by using the rownames() function. In this example, the mtcars dataset already has each rowname as a unique make/model of a car.

It also requires the input to be a matrix rather than a dataset
```{r}
mtcars_scaled <- mtcars %>% 
  mutate(across(everything(), scale))

rownames(mtcars_scaled) <- rownames(mtcars)

heatmap(as.matrix(mtcars_scaled))
```

### - Linear algebra in R

Creating a matrix 
```{r}
my_matrix <- matrix(c(1,2,3,4,5,6,7,8,9), nrow=3, byrow=TRUE)
my_matrix
```

Transpose 
```{r}
t(my_matrix)
```

Matrix multiplication
```{r, eval=FALSE}
matrix_1 %*% matrix_2
```

Trace
```{r, eval=FALSE}
sum(diag(my_matrix))
```

Determinant
```{r, eval=FALSE}
det(my_matrix)
```






















End